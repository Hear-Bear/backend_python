# main.py
# ëª¨ë“  ëª¨ë“ˆì„ ì„í¬íŠ¸í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•œë‹¤.

import argparse
import json
import os

import numpy as np
import torch
import torch.nn.functional as F
from sklearn.metrics import f1_score, accuracy_score

import config
from dataset import prepare_data, FSD50KSubsetDataset
from model import load_model_and_extractor
from test import test_single_audio
from train import train_model


def get_threshold_for_class(class_name):
    # í´ë˜ìŠ¤ ì´ë¦„ì„ ë°›ì•„ì„œ configì— ì„¤ì •ëœ ìµœì ì˜ ì„ê³„ê°’ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜

    # íŠ¹ì • í´ë˜ìŠ¤ ê°•ì œ ì„¤ì • í™•ì¸ (Override)
    if hasattr(config, 'CLASS_SPECIFIC_THRESHOLDS'):
        if class_name in config.CLASS_SPECIFIC_THRESHOLDS:
            return config.CLASS_SPECIFIC_THRESHOLDS[class_name]

    # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ í™•ì¸
    category = config.CLASS_CATEGORY_MAP.get(class_name, 'Default')

    # ì¹´í…Œê³ ë¦¬ë³„ ì„ê³„ê°’ ë°˜í™˜
    return config.CATEGORY_THRESHOLDS.get(category, 0.5)


def main(args):
    # ì „ì²´ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰.

    # ë°ì´í„° ì¤€ë¹„ (dataset.py)
    train_df, val_df, test_df, label2id, id2label, class_weights = prepare_data(
        config.FSD50K_DEV_CSV,
        config.FSD50K_EVAL_CSV,
        config.CLASS_GROUPS,
        config.FSD50K_DEV_AUDIO_DIR,
        config.FSD50K_EVAL_AUDIO_DIR,
        config.VALIDATION_SPLIT_SIZE,
        config.RANDOM_SEED
    )

    if train_df is None:
        print("ë°ì´í„° ì¤€ë¹„ì— ì‹¤íŒ¨í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # ëª¨ë¸ ë¡œë“œ (model.py)
    num_labels = len(config.CLASS_GROUPS)
    model, feature_extractor = load_model_and_extractor(
        config.MODEL_ID, num_labels, label2id, id2label
    )

    # ë°ì´í„°ì…‹ ë¡œë“œ
    train_dataset = FSD50KSubsetDataset(
        train_df,
        config.FSD50K_DEV_AUDIO_DIR,
        feature_extractor,
        use_augmentation=True  # Trueë¡œ ì„¤ì •í•˜ì—¬ ì¦ê°• ì‚¬ìš©
    )
    val_dataset = FSD50KSubsetDataset(
        val_df,
        config.FSD50K_DEV_AUDIO_DIR,
        feature_extractor,
        use_augmentation=False
    )
    test_dataset = FSD50KSubsetDataset(
        test_df,
        config.FSD50K_EVAL_AUDIO_DIR,
        feature_extractor,
        use_augmentation=False
    )

    # ëª¨ë¸ í•™ìŠµ (train.py)
    trainer = train_model(
        model=model,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        # feature_extractor=feature_extractor,
        output_dir=args.output_dir,
        learning_rate=args.learning_rate,
        num_train_epochs=args.num_train_epochs,
        batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        early_stopping_patience=args.early_stopping_patience,
        class_weights=class_weights,
    )

    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥
    best_model_dir = f"{args.output_dir}/best_model"
    print(f"í•™ìŠµ ì™„ë£Œ. ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ {best_model_dir}ì— ì €ì¥í•©ë‹ˆë‹¤.")

    trainer.save_model(best_model_dir)

    # í”¼ì²˜ ì¶”ì¶œê¸°ë„ í•¨ê»˜ ì €ì¥í•´ì•¼ ì¶”ë¡  ì‹œ ë™ì¼í•œ ì „ì²˜ë¦¬ ë³´ì¥
    feature_extractor.save_pretrained(best_model_dir)

    # # ìµœì  ì„ê³„ê°’ íŠœë‹(Validation Set ì‚¬ìš©)
    # print("\n--- ìµœì  ì„ê³„ê°’ íŠœë‹ (Validation Set) ---")
    #
    # # (val_datasetì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŒ)
    # val_predictions = trainer.predict(val_dataset)
    # val_logits = val_predictions.predictions
    # val_labels = val_predictions.label_ids
    #
    # # [ë””ë²„ê¹…] ë°ì´í„° í™•ì¸
    # print(f"[Debug] Logits Shape: {val_logits.shape}")
    # print(f"[Debug] Labels Shape: {val_labels.shape}")
    #
    # # NaN í™•ì¸
    # if np.isnan(val_logits).any():
    #     print("ğŸš¨ [ì¹˜ëª…ì  ì˜¤ë¥˜] Logitsì— NaN ê°’ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! í•™ìŠµì´ í­ë°œí–ˆìŠµë‹ˆë‹¤.")
    #     print("-> í•´ê²°ì±…: Learning Rateë¥¼ ë‚®ì¶”ê±°ë‚˜(1e-5), Gradient Accumulationì„ ì¤„ì´ì„¸ìš”.")
    # else:
    #     print(f"[Debug] Logits ì˜ˆì‹œ (Top 5): {val_logits[0][:5]}")
    #     print(f"[Debug] Logits Min: {val_logits.min()}, Max: {val_logits.max()}")
    #
    # # Labels í™•ì¸
    # print(f"[Debug] Labels ì˜ˆì‹œ (Top 5): {val_labels[0][:5]}")
    #
    # # NumPy -> Torch Tensorë¡œ ë³€í™˜
    # val_logits_tensor = torch.from_numpy(val_logits)
    # # Sigmoid ì ìš©
    # val_probs = F.sigmoid(val_logits_tensor).numpy()
    #
    # print(f"[Debug] Probabilities ì˜ˆì‹œ (Top 5): {val_probs[0][:5]}")
    #
    # best_threshold = 0.5
    # best_f1_macro = 0.0
    #
    # # 0.1ë¶€í„° 0.9ê¹Œì§€ 0.05 ìŠ¤í…ìœ¼ë¡œ íƒìƒ‰
    # threshold_candidates = np.arange(0.1, 0.9, 0.05)
    #
    # print("ì„ê³„ê°’ íƒìƒ‰ ì¤‘...")
    # for threshold in threshold_candidates:
    #     preds = (val_probs > threshold).astype(int)
    #     f1 = f1_score(val_labels, preds, average="macro", zero_division=0)
    #
    #     # ë§Œì•½ ëª¨ë“  ì˜ˆì¸¡ì´ 0ì´ë¼ë©´?
    #     if preds.sum() == 0:
    #         # print(f"ì„ê³„ê°’ {threshold:.2f}: ì˜ˆì¸¡ëœ Positiveê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.")
    #         pass
    #
    #     if f1 > best_f1_macro:
    #         best_f1_macro = f1
    #         best_threshold = threshold
    #
    # print(f"ìµœì  ì„ê³„ê°’: {best_threshold:.2f} (F1 Macro: {best_f1_macro:.4f})")

    print("\n--- ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ì„ê³„ê°’ ì ìš© ì¤€ë¹„ ---")

    # ëª¨ë¸ì˜ í´ë˜ìŠ¤ ê°œìˆ˜
    num_classes = len(id2label)

    # (í´ë˜ìŠ¤ ê°œìˆ˜,) í˜•íƒœì˜ ì„ê³„ê°’ ë°°ì—´ ìƒì„±
    manual_thresholds = np.zeros(num_classes)
    threshold_log = {}  # ë¡œê·¸ ì €ì¥ìš©

    print("ì ìš©ëœ ì„ê³„ê°’ (ì¼ë¶€):")
    for i in range(num_classes):
        class_name = id2label[i]

        # ì‘ì„±í•œ í•¨ìˆ˜ë¥¼ í†µí•´ ì„ê³„ê°’ ê°€ì ¸ì˜¤ê¸°
        th = get_threshold_for_class(class_name)

        manual_thresholds[i] = th
        threshold_log[class_name] = th

        # ë„ˆë¬´ ë§ìœ¼ë‹ˆ ì•ë¶€ë¶„ë§Œ ì¶œë ¥
        if i < 5:
            print(f" - {class_name}: {th}")

    # ë² ìŠ¤íŠ¸ ëª¨ë¸ë¡œ ì „ì²´ í‰ê°€ ë°ì´í„°ì…‹ ìµœì¢… í‰ê°€
    print("\n--- ë² ìŠ¤íŠ¸ ëª¨ë¸ ìµœì¢… í‰ê°€ ---")

    final_results = {
        "config": {
            "model_id": config.MODEL_ID,
            "epochs": args.num_train_epochs,
            "batch_size": args.batch_size,
            "threshold_type": "Category-based Model"
        }
    }

    if not test_df.empty:
        test_dataset = FSD50KSubsetDataset(
            test_df,
            config.FSD50K_EVAL_AUDIO_DIR,
            feature_extractor,
            use_augmentation=False
        )

        # ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
        test_predictions = trainer.predict(test_dataset)
        test_logits = test_predictions.predictions
        test_labels = test_predictions.label_ids

        # Sigmoid ì ìš© & ìµœì  ì„ê³„ê°’ìœ¼ë¡œ 0/1 ë³€í™˜
        test_logits_tensor = torch.from_numpy(test_logits)
        test_probs = F.sigmoid(test_logits_tensor).numpy()

        final_preds = (test_probs > manual_thresholds).astype(int)

        # [ì „ì²´ ì§€í‘œ] ê³„ì‚° ë° ì €ì¥
        test_metrics = {
            "accuracy_subset": accuracy_score(test_labels, final_preds),
            "f1_macro": f1_score(test_labels, final_preds, average="macro", zero_division=0),
            "f1_weighted": f1_score(test_labels, final_preds, average="weighted", zero_division=0)
        }
        final_results["test_metrics"] = test_metrics  # ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€

        # [í´ë˜ìŠ¤ë³„ ì§€í‘œ] ê³„ì‚° ë° ì €ì¥
        # average=Noneì„ ì£¼ë©´ ê° í´ë˜ìŠ¤ë³„ ì ìˆ˜ê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ë‚˜ì˜µë‹ˆë‹¤.
        per_class_f1 = f1_score(test_labels, final_preds, average=None, zero_division=0)

        class_performance = {}
        for i, score in enumerate(per_class_f1):
            class_name = id2label[i]  # ID(0) -> ì´ë¦„(Alarm) ë³€í™˜
            th = manual_thresholds[i]

            class_performance[class_name] = {
                'f1_score': float(score),
                'threshold': float(th),
            }

        # ì ìˆ˜ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ì„œ ì €ì¥
        sorted_performance = dict(sorted(class_performance.items(), key=lambda item: item[1]['f1_score'], reverse=True))
        final_results["per_class_f1"] = sorted_performance

        # í„°ë¯¸ë„ ì¶œë ¥
        print(f"ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼: {test_metrics}")
        print("ìƒìœ„ 5ê°œ í´ë˜ìŠ¤ ì„±ëŠ¥:", list(sorted_performance.items())[:5])
        print("í•˜ìœ„ 5ê°œ í´ë˜ìŠ¤ ì„±ëŠ¥:", list(sorted_performance.items())[-5:])

    else:
        print("Test Setì´ ì—†ì–´ ìµœì¢… í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        final_results["error"] = "No Test Set found"

    # íŒŒì¼ë¡œ ì €ì¥(JSON)
    save_path = os.path.join(args.output_dir, "final_results.json")

    # JSON íŒŒì¼ ì“°ê¸°
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(final_results, f, indent=4, ensure_ascii=False)

    print(f"\nëª¨ë“  í‰ê°€ ê²°ê³¼ê°€ '{save_path}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # í…ŒìŠ¤íŠ¸(ì¶”ë¡ ) ì‹¤í–‰ (test.py)
    print("\n--- ë‹¨ì¼ íŒŒì¼ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ---")
    if not test_df.empty:
        test_file_name = test_df.iloc[0]['fname']
        test_file_path = f"{config.FSD50K_EVAL_AUDIO_DIR}/{test_file_name}.wav"

        test_single_audio(
            model_path=best_model_dir,
            audio_file=test_file_path,
            target_sr=feature_extractor.sampling_rate,
            id2label=id2label
        )


# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    # 4. argparse íŒŒì„œ ì„¤ì •
    parser = argparse.ArgumentParser(description="AST ëª¨ë¸ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸")

    # config.pyì˜ ê°’ë“¤ì„ 'default'ë¡œ ì‚¬ìš©
    parser.add_argument(
        '--output_dir',
        type=str,
        default=config.OUTPUT_DIR,
        help='ëª¨ë¸ ì¶œë ¥ ë° ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬'
    )
    parser.add_argument(
        '--learning_rate',
        type=float,
        default=config.LEARNING_RATE,
        help='í•™ìŠµë¥ '
    )
    parser.add_argument(
        '--num_train_epochs',
        type=int,
        default=config.NUM_TRAIN_EPOCHS,
        help='ì´ í•™ìŠµ ì—í¬í¬ ìˆ˜'
    )
    parser.add_argument(
        '--batch_size',
        type=int,
        default=config.BATCH_SIZE_PER_DEVICE,
        help='ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°'
    )
    parser.add_argument(
        '--gradient_accumulation_steps',
        type=int,
        default=config.GRADIENT_ACCUMULATION_STEPS,
        help='ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì  ìŠ¤í…'
    )

    parser.add_argument(
        '--early_stopping_patience',
        type=int,
        default=config.EARLY_STOPPING_PATIENCE,
        help='ì¡°ê¸° ì¢…ë£Œ Paitence'
    )

    args = parser.parse_args()
    main(args)  # íŒŒì‹±ëœ ì¸ìë¥¼ main í•¨ìˆ˜ì— ì „ë‹¬